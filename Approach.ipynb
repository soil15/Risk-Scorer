{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating DataFrame from RAW data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<body>\n",
    "  <p>1. While downloading the data I chose all the fields for the first time, the file being JSON it has nested structure, first task would be to flatten this file into a dataframe.</p>\n",
    "  <p>2. The problem was not every study has all the information.<br>\n",
    "     &nbsp;&nbsp;&nbsp;&nbsp;Example:<br>\n",
    "     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;study A has fields F_1, F_2, F_3<br>\n",
    "     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;study B has fields F_1, F_2, F_3, F_4, F_5<br>\n",
    "     &nbsp;&nbsp;&nbsp;&nbsp;so after flattening these two studies into a single dataframe will have 5 columns with F_4 and F_5 being Null for study A.\n",
    "  </p>\n",
    "  <p>3. This is where downloading all the fields became a huge issue. Only after flattening and combining 5 studies my dataframe had 5000 columns.</p>\n",
    "  <p>4. So I downloaded the most popular fields for the studies, which after flattening and combining gave me 73 columns.</p>\n",
    "  <p>5. Now it seems good enough to carry on. But there is still one issue, after looking at the Null values 62 columns out of 73 columns had 80% Null values. Columns with such high number of Null values should be dropped so there are only 11 columns to work with.</p>\n",
    "  <p>6. So I decided to download the data again with most popular fields and some more additional fields which may provide some more information.</p>\n",
    "  <p>7. After downloding the custome data, I chose to drop fields with NULL Values higher than 10% because if we choose to replace them, thoes values should not create any synthetic data (Should not make major changes in skewness of the data) which may not resemble LIVE/ACTUAL data.</p>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **First Iteration (The Brain Dead Approach)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<body>\n",
    "  <p>1. In this one i wont be doing any kind of EDA, Faeture Enginnering, Feature Selection</p>\n",
    "  <p>2. Will just feed the data to model with minimum preparation</p>\n",
    "  <p>3. Won't be handling outlier (will be using Tree based models which are rosust to outliers), will not put much thought in feature encoding (will just label encode all categorical features), will ignore the class imbalance</p>\n",
    "  <p><b>Exceptations :</b></p> \n",
    "  &nbsp;&nbsp;1. The point of doing this is to just see whether model is able understand anykind of patterns in the data</br>\n",
    "  &nbsp;&nbsp;2. It is expected that this model will turn out to be the worst one possible, but this should give some idea whether models are easily overfitting or underfitting, what areas will need more attention etc.</br>\n",
    "  <p><b>Results :</b></p>\n",
    "    <p><B>Random Forest</B></p>\n",
    "    &nbsp;&nbsp;Precision : 0.698961937716263</br>\n",
    "    &nbsp;&nbsp;Recall : 0.698961937716263</br>\n",
    "    &nbsp;&nbsp;F1_score : 0.5787965616045846</br>\n",
    "    &nbsp;&nbsp;Trainning Accuracy : 1.0</br>\n",
    "    &nbsp;&nbsp;Testing Accuracy : 0.8292682926829268</br>\n",
    "    <p><B>Ada Boost</B></p>\n",
    "    &nbsp;&nbsp;Precision : 0.6120401337792643</br>\n",
    "    &nbsp;&nbsp;Recall : 0.4474327628361858</br>\n",
    "    &nbsp;&nbsp;F1_score : 0.516949152542373</br>\n",
    "    &nbsp;&nbsp;Trainning Accuracy : 1.0</br>\n",
    "    &nbsp;&nbsp;Testing Accuracy : 0.8013937282229965</br>\n",
    "    <p><B>XG Boost</B></p>\n",
    "    &nbsp;&nbsp;Precision : 0.6890756302521008</br>\n",
    "    &nbsp;&nbsp;Recall : 0.6014669926650367</br>\n",
    "    &nbsp;&nbsp;F1_score : 0.6422976501305483</br>\n",
    "    &nbsp;&nbsp;Trainning Accuracy : 1.0</br>\n",
    "    &nbsp;&nbsp;Testing Accuracy : 0.8408826945412311</br>\n",
    "    <p><B>SVC</B></p>\n",
    "    &nbsp;&nbsp;Precision : 0.0</br>\n",
    "    &nbsp;&nbsp;Recall : 0.0</br>\n",
    "    &nbsp;&nbsp;F1_score : 0.0</br>\n",
    "    &nbsp;&nbsp;Trainning Accuracy : 1.0</br>\n",
    "    &nbsp;&nbsp;Testing Accuracy : 0.7624854819976771</br>\n",
    "  <p><b>Observations :</b></p>\n",
    "    1. The models are overfitting.</br>\n",
    "    2. The effect of class imbalance is clearly visible as there is huge difference between precision and recall scores   and this is the reason why F1_score is way less than the testing Accuracy.</br>\n",
    "    3. XG boost seems to be immune to class imbalance as it has minimum difference between its precision and recall scores.</br>\n",
    "    4. Also, XG Boost has the highest F1_score among all the other models.</br>\n",
    "    5. SVC did not do well at all which was expected since its a linear model and we did not do any kind of preprocessing such as handling the outliers, scaling the data as well, also it is possible that the classes themselves are not linearly seperable, still in that case we can still use Kernels of SVC if we see any sign of classes being lienarly seperable in EDA in next iteration.</br>\n",
    "  <p><b>Next Goals :</b></p>\n",
    "    1. Create a model with maximum possible F1_score.</br>\n",
    "    2. Try to reduce the overfitting.</br>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Second Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please go through the folder notebooks/second_iteration \n",
    "for details"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
